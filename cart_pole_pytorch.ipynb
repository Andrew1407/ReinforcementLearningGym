{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height: int, img_width: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=img_height * img_width * 3, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(typename='Experience', field_names=('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.__memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, experience: Experience):\n",
    "        self.__memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> list[Experience]:\n",
    "        return random.sample(self.__memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size: int) -> bool:\n",
    "        return len(self) >= batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.__memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy:\n",
    "    def __init__(self, start: float, end: float, decay: float):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step: int) -> float:\n",
    "        return EpsilonGreedyStrategy.exploration_rate(self.start, self.end, self.decay, current_step)\n",
    "    \n",
    "    def __call__(self, current_step: int) -> float:\n",
    "        return self.get_exploration_rate(current_step)\n",
    "\n",
    "    @staticmethod\n",
    "    def exploration_rate(start: float, end: float, decay: float, current_step: int):\n",
    "        return end + (start - end) * math.exp(-1 * current_step * decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, strategy: EpsilonGreedyStrategy, num_actions: int, device: torch.device):\n",
    "        self.cur_step: int = 0\n",
    "        self.startegy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state: torch.Tensor, policy_net: DQN) -> torch.Tensor:\n",
    "        rate = self.startegy.get_exploration_rate(self.cur_step)\n",
    "        self.cur_step += 1\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax(dim=1).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnvManager:\n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('CartPole-v1', render_mode='human').unwrapped\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    def render(self, mode: str = 'human') -> np.ndarray:\n",
    "        self.env.render_mode = mode\n",
    "        return self.env.render()\n",
    "    \n",
    "    def num_actions_available(self) -> int:\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def take_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        new_state, reward, done, truncated, info = self.env.step(action.item())\n",
    "        self.done = done\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "\n",
    "    def just_starting(self) -> bool:\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self) -> torch.Tensor:\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            back_screen = torch.zeros_like(self.current_screen)\n",
    "            return back_screen\n",
    "        s1 = self.current_screen\n",
    "        s2 = self.get_processed_screen()\n",
    "        self.current_screen = s2\n",
    "        return s1 - s2\n",
    "\n",
    "    def get_screen_height(self) -> int:\n",
    "        return self.get_processed_screen().shape[2]\n",
    "    \n",
    "    def get_screen_width(self) -> int:\n",
    "        return self.get_processed_screen().shape[3]\n",
    "    \n",
    "    def get_processed_screen(self) -> torch.Tensor:\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1))\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen: np.ndarray) -> np.ndarray:\n",
    "        screen_height = screen.shape[1]\n",
    "        \n",
    "        top = int(screen_height * .4)\n",
    "        bottom = int(screen_height * .8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    \n",
    "    def transform_screen_data(self, screen: np.ndarray) -> torch.Tensor:\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((40, 90)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        return resize(screen).unsqueeze(0).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net: DQN, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net: DQN, next_states: torch.Tensor) -> torch.Tensor:\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        t = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        values[non_final_state_locations] = t\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) < period:\n",
    "        return torch.zeros(len(values)).numpy()\n",
    "    moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "    moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
    "    return moving_avg.numpy()\n",
    "\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "    moving_average = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_average)\n",
    "    plt.pause(.001)\n",
    "    print('Episode', len(values), '\\n', moving_avg_period, 'episode moving avg:', moving_average[-1])\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences: list[Experience]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    return t1, t2, t3, t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = .999\n",
    "eps_start = 1\n",
    "eps_end = .01\n",
    "eps_decay = .001\n",
    "target_update = 10\n",
    "memory_size = 100_000\n",
    "lr = .001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em = CartPoleEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = list()\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    for timestep in count():\n",
    "        em.render()\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            plot(episode_durations, 100)\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
